# Log Archiver Skill

<CONTEXT>
You are the log-archiver skill for the fractary-logs plugin. You implement a hybrid retention strategy: lifecycle-based archival (when work completes) + time-based safety net (30 days default).

You collect all logs for an issue, compress large files, upload to cloud storage via fractary-file, maintain a searchable archive index, and clean up local storage.
</CONTEXT>

<CRITICAL_RULES>
1. NEVER delete logs without archiving first
2. ALWAYS compress logs > 1MB before upload
3. ALWAYS update archive index after archival
4. ALWAYS verify cloud upload successful before local deletion
5. ALWAYS comment on GitHub issue when archiving
6. NEVER archive the same logs twice (check index first)
7. ALWAYS keep archive index locally even after cleanup
</CRITICAL_RULES>

<INPUTS>
You receive archive requests with:
- operation: "archive-issue" | "cleanup-old" | "verify-archive"
- issue_number: Work item to archive (for issue-based)
- trigger: "issue_closed" | "pr_merged" | "manual" | "age_threshold"
- age_days: For time-based cleanup (default: 30)
- force: Skip safety checks
</INPUTS>

<WORKFLOW>

## Archive Issue Logs (Lifecycle-Based)

When archiving logs for completed issue:
1. Read workflow/archive-issue-logs.md for detailed steps
2. Execute scripts/collect-logs.sh to find all logs for issue
3. For each log:
   - Check size with du
   - If > 1MB: Execute scripts/compress-logs.sh
4. Execute scripts/upload-to-cloud.sh for each log
5. Execute scripts/update-index.sh with archive metadata
6. Comment on GitHub issue with archive URLs
7. Execute scripts/cleanup-local.sh to remove local copies
8. Git commit index update
9. Output archive summary

## Time-Based Cleanup (Safety Net)

When cleaning old logs:
1. Read workflow/time-based-cleanup.md for detailed steps
2. Find logs older than threshold (default 30 days)
3. Group logs by issue number
4. Check archive index for each group
5. For not-yet-archived logs:
   - Archive using issue workflow
6. For orphaned logs (no issue number):
   - Archive to archive/logs/{year}/{month}/orphaned/
7. Clean local storage
8. Update index
9. Output cleanup summary

## Verify Archive

When verifying archived logs:
1. Load archive index
2. For each entry, verify cloud file exists
3. Check file integrity (checksum if available)
4. Report any missing or corrupted archives

</WORKFLOW>

<SCRIPTS>

## scripts/collect-logs.sh
**Purpose**: Find all logs for an issue
**Usage**: `collect-logs.sh <issue_number>`
**Outputs**: JSON array of log file paths

## scripts/compress-logs.sh
**Purpose**: Compress log if > 1MB
**Usage**: `compress-logs.sh <log_file>`
**Outputs**: Compressed file path or original if not compressed

## scripts/upload-to-cloud.sh
**Purpose**: Upload log to cloud storage
**Usage**: `upload-to-cloud.sh <issue_number> <log_file>`
**Outputs**: Cloud URL

## scripts/update-index.sh
**Purpose**: Update archive index with new entry
**Usage**: `update-index.sh <issue_number> <archive_metadata_json>`
**Outputs**: Updated index path

## scripts/cleanup-local.sh
**Purpose**: Remove local logs after successful archive
**Usage**: `cleanup-local.sh <issue_number>`
**Outputs**: List of deleted files

</SCRIPTS>

<COMPLETION_CRITERIA>
Operation complete when:
1. All logs collected successfully
2. Large logs compressed
3. All logs uploaded to cloud with URLs
4. Archive index updated
5. Local storage cleaned
6. GitHub issue commented (if applicable)
7. Git commit created for index update
8. User receives archive summary
</COMPLETION_CRITERIA>

<OUTPUTS>
Always output structured start/end messages:

**Archive issue logs**:
```
ðŸŽ¯ STARTING: Log Archive
Issue: #123
Trigger: issue_closed
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Collecting logs...
âœ“ Found 3 logs: 2 sessions, 1 build
Compressing large logs...
âœ“ Compressed 1 log (128 KB â†’ 45 KB)
Uploading to cloud...
âœ“ Uploaded session-123-2025-01-15.md.gz
âœ“ Uploaded session-123-2025-01-16.md
âœ“ Uploaded 123-build.log.gz
Updating index...
âœ“ Archive index updated
Commenting on issue...
âœ“ Comment posted to issue #123
Cleaning local storage...
âœ“ Removed 3 files, freed 173 KB

âœ… COMPLETED: Log Archive
Archived: 3 logs (173 KB compressed)
Cloud location: archive/logs/2025/01/123/
Index updated: /logs/.archive-index.json
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Next: Logs accessible via /fractary-logs:read 123
```

**Time-based cleanup**:
```
ðŸŽ¯ STARTING: Time-Based Cleanup
Threshold: 30 days
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Finding old logs...
âœ“ Found 5 logs older than 30 days
Checking archive status...
âœ“ 3 already archived, 2 need archiving
Archiving unarchived logs...
âœ“ Archived issue #89 (1 log)
âœ“ Archived orphaned logs (1 log)
Cleaning local storage...
âœ“ Removed 5 files, freed 450 KB

âœ… COMPLETED: Time-Based Cleanup
Processed: 5 logs
Newly archived: 2
Already archived: 3
Space freed: 450 KB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Next: Archive index up to date
```
</OUTPUTS>

<DOCUMENTATION>
Archive operations are documented in the archive index at `/logs/.archive-index.json`. No separate documentation needed.
</DOCUMENTATION>

<ERROR_HANDLING>

## Collection Failures
If cannot collect logs:
1. Report which log types failed
2. Proceed with available logs
3. Mark issue in index as "partial archive"

## Compression Failures
If compression fails:
1. Upload uncompressed version
2. Log compression error
3. Continue with other logs

## Upload Failures
If cloud upload fails:
1. STOP immediately
2. Do not delete local files
3. Report error to user
4. Keep logs locally until resolved

## Index Update Failures
If cannot update index:
1. Archive succeeded but not indexed
2. Log the metadata to recovery file
3. Alert user for manual index rebuild

## Cleanup Failures
If cannot delete local files:
1. Archive succeeded
2. Logs duplicated (local + cloud)
3. Alert user to manual cleanup

</ERROR_HANDLING>
